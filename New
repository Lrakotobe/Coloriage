import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.Configuration;
import org.apache.spark.sql.SparkSession;
import scala.collection.JavaConversions;

import java.io.IOException;
import java.util.List;

public class RenameS3ObjectWithSpark {

    public static void main(String[] args) throws IOException {
        String bucketName = "votre-bucket";
        String directory = "s3a://" + bucketName + "/chemin/nom/";
        String newFilePath = "s3a://" + bucketName + "/chemin/nom.csv";

        SparkSession spark = SparkSession.builder()
                .appName("RenameS3Object")
                .getOrCreate();

        // Lister les fichiers dans le répertoire S3
        List<String> fileList = JavaConversions.seqAsJavaList(
                spark.read().format("csv").load(directory).inputFiles()
        );

        Configuration hadoopConf = spark.sparkContext().hadoopConfiguration();
        FileSystem fs = FileSystem.get(hadoopConf);

        for (String filePath : fileList) {
            if (filePath.endsWith(".csv")) {
                Path oldPath = new Path(filePath);
                Path newPath = new Path(newFilePath);

                // Copier l'objet vers le nouveau nom
                if (fs.rename(oldPath, newPath)) {
                    System.out.println("Renommage réussi: " + oldPath + " vers " + newPath);
                } else {
                    System.err.println("Échec du renommage de: " + oldPath);
                }

                // Si un seul fichier doit être renommé, on peut sortir de la boucle après le renommage
                break;
            }
        }

        spark.stop();
    }
}
